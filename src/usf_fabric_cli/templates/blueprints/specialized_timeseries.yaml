# ─────────────────────────────────────────────────────────────────────────────
# Specialized Time Series Workspace Configuration Template
# Optimized for: IoT telemetry, financial tick data, sensor analytics
# Use Cases: Manufacturing monitoring, energy grid analytics, financial markets
# Strategy: Git-sync-only — Fabric items managed through Git, not CLI
# ─────────────────────────────────────────────────────────────────────────────

workspace:
  name: "timeseries-workspace"        # ← CHANGE: Your project workspace name
  display_name: "Time Series Analytics Platform"
  description: "High-frequency time series ingestion, storage, and analysis"
  capacity_id: "${FABRIC_CAPACITY_ID}"
  domain: "${FABRIC_DOMAIN_NAME}"

  # Git Integration
  git_repo: "${GIT_REPO_URL}"
  git_branch: "main"
  git_directory: "/"

environments:
  dev:
    workspace:
      name: "timeseries-workspace"
      capacity_id: "${FABRIC_CAPACITY_ID}"
  test:
    workspace:
      capacity_id: "${FABRIC_CAPACITY_ID}"
  prod:
    workspace:
      capacity_id: "${FABRIC_CAPACITY_ID}"

# ── Folder Structure (Numbered Convention) ───────────────────────────────────
folders:
  - "000 Orchestrate"
  - "100 Ingest"
  - "200 Store"
  - "300 Prepare"
  - "400 Model"
  - "500 Visualize"
  - "999 Libraries"
  - "Archive"

# ── Folder Rules (Post-Git-Sync Item Placement) ─────────────────────────────
folder_rules:
  - type: DataPipeline
    folder: "000 Orchestrate"
  - type: DataflowGen2
    folder: "000 Orchestrate"
  - type: Eventstream
    folder: "100 Ingest"
  - type: Lakehouse
    folder: "200 Store"
  - type: Notebook
    folder: "300 Prepare"
  - type: SparkJobDefinition
    folder: "300 Prepare"
  - type: Warehouse
    folder: "400 Model"
  - type: SemanticModel
    folder: "400 Model"
  - type: Report
    folder: "500 Visualize"
  - type: Dashboard
    folder: "500 Visualize"
  - type: Environment
    folder: "999 Libraries"

# ── No Fabric Items — Content Managed Through Git Sync ───────────────────────
# Typical Git-managed items for a time series project:
#   - Eventstream: sensor_telemetry (100 Ingest) — high-frequency sensor data
#   - Eventstream: market_tick_data (100 Ingest) — financial tick stream
#   - KQLDatabase: realtime_telemetry_db (200 Store) — hot queries (<24h)
#   - KQLDatabase: recent_history_db (200 Store) — warm data (7-30 days)
#   - Lakehouse: historical_archive (200 Store) — cold storage (years)
#   - Notebook: ts_anomaly_detection (300 Prepare) — anomaly algorithms
#   - Notebook: ts_aggregation (300 Prepare) — downsample & rollup
#   - Notebook: ts_forecasting (300 Prepare) — predictive models
#   - Notebook: data_retention_manager (300 Prepare) — TTL enforcement
#   - Pipeline: realtime_ingestion (000 Orchestrate)
#   - Pipeline: historical_rollup (000 Orchestrate) — daily aggregation
#   - Pipeline: archive_compaction (000 Orchestrate) — cold tier management
#   - Reflex: alert_anomaly (100 Ingest) — threshold alerting
#   - Reflex: alert_data_gap (100 Ingest) — missing data detection
#   - Reflex: alert_quality (100 Ingest) — data quality monitoring
#   - Report: operations_dashboard (500 Visualize) — real-time ops view
# ─────────────────────────────────────────────────────────────────────────────
lakehouses: []
notebooks: []
resources: []

# ── Access Control ───────────────────────────────────────────────────────────
principals:
  # 1. Automation Service Principal (runs deployments)
  - id: "${AZURE_CLIENT_ID}"
    role: Contributor
    description: "Automation SP — required for CI/CD deployments"

  # 2. Mandatory Admin security group (IT governance)
  - id: "${ADDITIONAL_ADMIN_PRINCIPAL_ID}"
    role: Admin
    description: "Mandatory governance admin group"

  # 3. Mandatory Contributor security group (support team)
  - id: "${ADDITIONAL_CONTRIBUTOR_PRINCIPAL_ID}"
    role: Contributor
    description: "Mandatory governance contributor group"

  # ── Project-specific principals ────────────────────────────────────────────
  - id: "${PROJECT_ADMIN_ID}"               # ← CHANGE
    role: Admin
    description: "Time series platform admins"

  - id: "${PROJECT_MEMBERS_ID}"             # ← CHANGE
    role: Member
    description: "IoT/streaming engineers"

  # Optional: Operations team with dashboard access
  # - id: "${OPS_TEAM_ID}"
  #   role: Viewer
  #   description: "Operations team — real-time dashboard access"

# ── Deployment Pipeline ──────────────────────────────────────────────────────
deployment_pipeline:
  pipeline_name: "timeseries-pipeline"   # ← CHANGE
  stages:
    development:
      workspace_name: "timeseries-workspace"
      capacity_id: "${FABRIC_CAPACITY_ID}"
    test:
      workspace_name: "timeseries-workspace [Test]"
      capacity_id: "${FABRIC_CAPACITY_ID_TEST:-FABRIC_CAPACITY_ID}"
    production:
      workspace_name: "timeseries-workspace [Production]"
      capacity_id: "${FABRIC_CAPACITY_ID_PROD:-FABRIC_CAPACITY_ID}"

# TIME SERIES ARCHITECTURE GUIDANCE:
# ─────────────────────────────────────────────────────────────────────────────
# Data Temperature Tiers:
#   Hot   (KQL DB): Last 24 hours — sub-second queries, real-time dashboards
#   Warm  (KQL DB): 7-30 days — interactive analysis, trend detection
#   Cold  (Lakehouse): 30+ days — historical analysis, compliance archive
#
# KQL Best Practices:
#   - Use materialized views for common aggregations
#   - Set retention policies per table (hot=1d, warm=30d)
#   - Use update policies for real-time transformations
#
# Eventstream Configuration:
#   - Sources: IoT Hub, Event Hubs, Kafka, Custom connectors
#   - Destinations: KQL DB (real-time), Lakehouse (batch archive)
#   - Transformations: Filter, aggregate, window operations
#
# Capacity Considerations:
#   - KQL databases consume CU based on data volume and query load
#   - Eventstreams consume CU based on throughput (events/sec)
#   - Consider F16+ for production time series workloads
