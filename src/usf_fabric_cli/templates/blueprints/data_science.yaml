# ─────────────────────────────────────────────────────────────────────────────
# Data Science Workspace Configuration Template
# Optimized for: ML research, experimentation, model development
# Use Cases: Research teams, ML model development, experiment tracking
# Strategy: Git-sync-only — Fabric items managed through Git, not CLI
# ─────────────────────────────────────────────────────────────────────────────

workspace:
  name: "data-science-lab"            # ← CHANGE: Your project workspace name
  display_name: "Data Science Lab"
  description: "Machine learning research and experimentation workspace"
  capacity_id: "${FABRIC_CAPACITY_ID}"
  domain: "${FABRIC_DOMAIN_NAME}"

  # Git Integration
  git_repo: "${GIT_REPO_URL}"
  git_branch: "main"
  git_directory: "/"

environments:
  dev:
    workspace:
      name: "data-science-lab"
      capacity_id: "${FABRIC_CAPACITY_ID}"
  test:
    workspace:
      capacity_id: "${FABRIC_CAPACITY_ID}"
  prod:
    workspace:
      capacity_id: "${FABRIC_CAPACITY_ID}"

# ── Folder Structure (Numbered Convention) ───────────────────────────────────
folders:
  - "000 Orchestrate"
  - "100 Ingest"
  - "200 Store"
  - "300 Prepare"
  - "400 Model"
  - "500 Visualize"
  - "999 Libraries"
  - "Archive"

# ── Folder Rules (Post-Git-Sync Item Placement) ─────────────────────────────
folder_rules:
  - type: DataPipeline
    folder: "000 Orchestrate"
  - type: DataflowGen2
    folder: "000 Orchestrate"
  - type: Eventstream
    folder: "100 Ingest"
  - type: Lakehouse
    folder: "200 Store"
  - type: Notebook
    folder: "300 Prepare"
  - type: SparkJobDefinition
    folder: "300 Prepare"
  - type: Warehouse
    folder: "400 Model"
  - type: SemanticModel
    folder: "400 Model"
  - type: Report
    folder: "500 Visualize"
  - type: Dashboard
    folder: "500 Visualize"
  - type: Environment
    folder: "999 Libraries"

# ── No Fabric Items — Content Managed Through Git Sync ───────────────────────
# Typical Git-managed items for a data science project:
#   - Lakehouse: research_data (200 Store) — training datasets, features
#   - Lakehouse: experiment_artifacts (200 Store) — model outputs, metrics
#   - Notebook: data_exploration (300 Prepare) — EDA and profiling
#   - Notebook: feature_engineering (300 Prepare) — feature computation
#   - Notebook: model_training (300 Prepare) — ML model training
#   - Notebook: model_evaluation (300 Prepare) — metrics and comparisons
#   - Pipeline: experiment_pipeline (000 Orchestrate) — automated ML runs
#   - Environment: ml_environment (999 Libraries) — custom Spark/Python deps
# ─────────────────────────────────────────────────────────────────────────────
lakehouses: []
notebooks: []
resources: []

# ── Access Control ───────────────────────────────────────────────────────────
principals:
  # 1. Automation Service Principal (runs deployments)
  - id: "${AZURE_CLIENT_ID}"
    role: Contributor
    description: "Automation SP — required for CI/CD deployments"

  # 2. Mandatory Admin security group (IT governance)
  - id: "${ADDITIONAL_ADMIN_PRINCIPAL_ID}"
    role: Admin
    description: "Mandatory governance admin group"

  # 3. Mandatory Contributor security group (support team)
  - id: "${ADDITIONAL_CONTRIBUTOR_PRINCIPAL_ID}"
    role: Contributor
    description: "Mandatory governance contributor group"

  # ── Project-specific principals ────────────────────────────────────────────
  - id: "${PROJECT_ADMIN_ID}"               # ← CHANGE
    role: Admin
    description: "Research lead / data science manager"

  - id: "${PROJECT_MEMBERS_ID}"             # ← CHANGE
    role: Member
    description: "Data scientists and ML engineers"

# ── Deployment Pipeline ──────────────────────────────────────────────────────
deployment_pipeline:
  pipeline_name: "data-science-pipeline"     # ← CHANGE
  stages:
    development:
      workspace_name: "data-science-lab"
      capacity_id: "${FABRIC_CAPACITY_ID}"
    test:
      workspace_name: "data-science-lab [Test]"
      capacity_id: "${FABRIC_CAPACITY_ID_TEST:-FABRIC_CAPACITY_ID}"
    production:
      workspace_name: "data-science-lab [Production]"
      capacity_id: "${FABRIC_CAPACITY_ID_PROD:-FABRIC_CAPACITY_ID}"

# DATA SCIENCE WORKFLOW GUIDANCE:
# ─────────────────────────────────────────────────────────────────────────────
# Experiment Lifecycle:
#   1. Explore data in exploration notebook (300 Prepare)
#   2. Engineer features and store in feature lakehouse (200 Store)
#   3. Train models in training notebook, log to MLflow
#   4. Evaluate and compare in evaluation notebook
#   5. Promote champion model to production pipeline
#
# MLflow Integration:
#   - Fabric natively supports MLflow experiment tracking
#   - Use `mlflow.autolog()` in training notebooks
#   - Compare runs in Fabric experiment viewer
#
# Environment (999 Libraries):
#   - Pin Spark/Python library versions for reproducibility
#   - Use Fabric Environments to manage custom dependencies
