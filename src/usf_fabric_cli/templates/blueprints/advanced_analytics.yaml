# ─────────────────────────────────────────────────────────────────────────────
# Advanced Analytics Workspace Configuration Template
# Optimized for: Complex analytics with ML, feature engineering, experiments
# Use Cases: Cross-functional analytics, ML pipelines, advanced BI, data mesh
# Strategy: Git-sync-only — Fabric items managed through Git, not CLI
# ─────────────────────────────────────────────────────────────────────────────

workspace:
  name: "advanced-analytics"          # ← CHANGE: Your project workspace name
  display_name: "Advanced Analytics Platform"
  description: "Cross-functional analytics platform with ML and advanced BI"
  capacity_id: "${FABRIC_CAPACITY_ID}"
  domain: "${FABRIC_DOMAIN_NAME}"

  # Git Integration
  git_repo: "${GIT_REPO_URL}"
  git_branch: "main"
  git_directory: "/"

environments:
  dev:
    workspace:
      name: "advanced-analytics"
      capacity_id: "${FABRIC_CAPACITY_ID}"
  test:
    workspace:
      capacity_id: "${FABRIC_CAPACITY_ID}"
  prod:
    workspace:
      capacity_id: "${FABRIC_CAPACITY_ID}"

# ── Folder Structure (Numbered Convention) ───────────────────────────────────
folders:
  - "000 Orchestrate"
  - "100 Ingest"
  - "200 Store"
  - "300 Prepare"
  - "400 Model"
  - "500 Visualize"
  - "999 Libraries"
  - "Archive"

# ── Folder Rules (Post-Git-Sync Item Placement) ─────────────────────────────
folder_rules:
  - type: DataPipeline
    folder: "000 Orchestrate"
  - type: DataflowGen2
    folder: "000 Orchestrate"
  - type: Eventstream
    folder: "100 Ingest"
  - type: Lakehouse
    folder: "200 Store"
  - type: Notebook
    folder: "300 Prepare"
  - type: SparkJobDefinition
    folder: "300 Prepare"
  - type: Warehouse
    folder: "400 Model"
  - type: SemanticModel
    folder: "400 Model"
  - type: Report
    folder: "500 Visualize"
  - type: Dashboard
    folder: "500 Visualize"
  - type: Environment
    folder: "999 Libraries"

# ── No Fabric Items — Content Managed Through Git Sync ───────────────────────
# Typical Git-managed items for advanced analytics:
#   - Lakehouse: raw_data_lake (200 Store) — raw ingested data
#   - Lakehouse: feature_store (200 Store) — engineered ML features
#   - Lakehouse: curated_analytics (200 Store) — business-ready datasets
#   - Lakehouse: experiment_data (200 Store) — ML experiment artifacts
#   - Warehouse: analytics_dw (400 Model) — star schema reporting
#   - Warehouse: ml_serving_dw (400 Model) — ML model predictions storage
#   - Notebook: feature_engineering (300 Prepare) — feature computation
#   - Notebook: model_training (300 Prepare) — ML model training
#   - Notebook: model_evaluation (300 Prepare) — model performance eval
#   - Notebook: data_quality_checks (300 Prepare) — automated DQ
#   - Notebook: advanced_transformations (300 Prepare)
#   - Notebook: ab_test_analysis (300 Prepare)
#   - Pipeline: daily_analytics_pipeline (000 Orchestrate)
#   - Pipeline: ml_training_pipeline (000 Orchestrate)
#   - Pipeline: feature_refresh_pipeline (000 Orchestrate)
#   - SemanticModel: executive_analytics_model (400 Model)
#   - Report: analytics_dashboard (500 Visualize)
# ─────────────────────────────────────────────────────────────────────────────
lakehouses: []
notebooks: []
resources: []

# ── Access Control ───────────────────────────────────────────────────────────
principals:
  # 1. Automation Service Principal (runs deployments)
  - id: "${AZURE_CLIENT_ID}"
    role: Contributor
    description: "Automation SP — required for CI/CD deployments"

  # 2. Mandatory Admin security group (IT governance)
  - id: "${ADDITIONAL_ADMIN_PRINCIPAL_ID}"
    role: Admin
    description: "Mandatory governance admin group"

  # 3. Mandatory Contributor security group (support team)
  - id: "${ADDITIONAL_CONTRIBUTOR_PRINCIPAL_ID}"
    role: Contributor
    description: "Mandatory governance contributor group"

  # ── Project-specific principals ────────────────────────────────────────────
  - id: "${PROJECT_ADMIN_ID}"               # ← CHANGE
    role: Admin
    description: "Analytics platform admins"

  - id: "${PROJECT_MEMBERS_ID}"             # ← CHANGE
    role: Member
    description: "Data engineers and ML engineers"

  # Optional: Data scientists with read access
  # - id: "${DATA_SCIENTISTS_ID}"
  #   role: Viewer
  #   description: "Data scientists — experiment access"

  # Optional: Business analysts with report access
  # - id: "${ANALYSTS_ID}"
  #   role: Viewer
  #   description: "Business analysts — report consumers"

# ── Deployment Pipeline ──────────────────────────────────────────────────────
deployment_pipeline:
  pipeline_name: "advanced-analytics-pipeline"   # ← CHANGE
  stages:
    development:
      workspace_name: "advanced-analytics"
      capacity_id: "${FABRIC_CAPACITY_ID}"
    test:
      workspace_name: "advanced-analytics [Test]"
      capacity_id: "${FABRIC_CAPACITY_ID}"
    production:
      workspace_name: "advanced-analytics [Production]"
      capacity_id: "${FABRIC_CAPACITY_ID}"

# ADVANCED ANALYTICS GUIDANCE:
# ─────────────────────────────────────────────────────────────────────────────
# Feature Store Pattern:
#   - Centralize engineered features in a dedicated lakehouse
#   - Version features with timestamps for reproducibility
#   - Share features across multiple ML models
#
# Experiment Tracking:
#   - Use MLflow integration via Fabric notebooks
#   - Store experiment artifacts in experiment_data lakehouse
#   - Compare model versions before promoting to production
#
# Data Quality:
#   - Run quality checks before promoting data between layers
#   - Use Great Expectations or custom quality notebooks
#   - Gate deployments on quality thresholds
