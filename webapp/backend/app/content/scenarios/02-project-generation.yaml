# Project Generation Scenario
# Comprehensive guide to all 10 templates with exact commands and outputs

id: project-generation
title: Generate Project Configurations from Templates
description: |
  Master the 10 production-ready blueprint templates. Learn the exact `python -m usf_fabric_cli.scripts.dev.generate_project`
  command syntax, understand when to use each template, and customize configurations for your organization's needs.
difficulty: beginner
estimated_duration_minutes: 30
category: configuration
order: 2

prerequisites:
  - getting-started

learning_outcomes:
  - "Use `python -m usf_fabric_cli.scripts.dev.generate_project` with all available options"
  - Choose the right template for your use case from all 10 blueprints
  - Understand the generated YAML configuration structure
  - Customize configurations for your organization
  - Use environment variable substitution in configurations
  - "Validate generated configs with `make validate`"

tags:
  - blueprints
  - templates
  - configuration
  - project-scaffolding
  - yaml
  - generate
  - generate_project.py

related_scenarios:
  - local-deployment
  - git-integration
  - environment-promotion

steps:
  - id: overview
    title: Blueprint Templates Overview
    type: info
    content: |
      The framework includes **10 production-ready blueprint templates** located in `src/usf_fabric_cli/templates/blueprints/`.
      Instead of writing YAML from scratch, you generate configurations using the `generate_project.py` script.

      ## The 10 Available Templates

      | Template | Complexity | Best For | Est. Monthly Cost |
      |----------|------------|----------|-------------------|
      | `minimal_starter` | â˜…â˜†â˜†â˜†â˜† | Learning, POCs | $0 (F2 trial) |
      | `basic_etl` | â˜…â˜…â˜†â˜†â˜† | Standard ETL pipelines | $100-500 |
      | `data_science` | â˜…â˜…â˜†â˜†â˜† | Research, ML experiments | $200-800 |
      | `advanced_analytics` | â˜…â˜…â˜…â˜†â˜† | ML/AI production | $500-1500 |
      | `realtime_streaming` | â˜…â˜…â˜…â˜…â˜† | IoT, event streaming | $800-2500 |
      | `data_mesh_domain` | â˜…â˜…â˜…â˜…â˜† | Domain-driven design | $1000-3000 |
      | `migration_hybrid` | â˜…â˜…â˜…â˜…â˜† | Cloud migration | $500-2000 |
      | `specialized_timeseries` | â˜…â˜…â˜…â˜…â˜† | Time-series analytics | $600-2000 |
      | `extensive_example` | â˜…â˜…â˜…â˜…â˜† | Reference architecture | $1000-3000 |
      | `compliance_regulated` | â˜…â˜…â˜…â˜…â˜… | HIPAA, Finance, Gov | $1500-5000 |

      ## Quick Start Recommendation

      - "**New to Fabric?** â†’ Start with `minimal_starter`"
      - "**Production ETL?** â†’ Use `basic_etl` (most common choice)"
      - "**Real-time data?** â†’ Use `realtime_streaming`"
      - "**Regulated industry?** â†’ Use `compliance_regulated`"
    tips:
      - All templates use environment variable substitution for credentials
      - Templates can be customized after generation
      - "View template source: `cat src/usf_fabric_cli/templates/blueprints/<template_name>.yaml`"

  - id: generate-command-syntax
    title: "Step 1: The generate_project.py Command"
    type: command
    content: |
      The `generate_project.py` script is your primary tool for creating configurations.

      ## Command Syntax

      ```bash
      python -m usf_fabric_cli.scripts.dev.generate_project "Organization Name" "Project Name" [OPTIONS]
      ```

      ## All Available Options

      | Option | Description | Default |
      |--------|-------------|---------|
      | `--template`, `-t` | Template name (see list above) | `basic_etl` |
      | `--capacity-id` | Fabric capacity ID | `${FABRIC_CAPACITY_ID}` |
      | `--git-repo` | Git repository URL | `${GIT_REPO_URL}` |
      | `--git-branch` | Git branch name | `main` |
      | `--output-dir` | Output directory | `config/projects/` |
      | `--force` | Overwrite existing | `False` |

      ## Output Location

      Generated files go to: `config/projects/{org_slug}/{project_slug}.yaml`

      - "\"Acme Corp\" â†’ `acme_corp`"
      - "\"Sales Analytics\" â†’ `sales_analytics`"
      - "Result: `config/projects/acme_corp/sales_analytics.yaml`"
    code:
      language: bash
      content: |
        # See all available options
        python -m usf_fabric_cli.scripts.dev.generate_project --help

        # List all available templates
        ls src/usf_fabric_cli/templates/blueprints/
    expected_output: |
      # --help output:
      Usage: generate_project.py [OPTIONS] ORG_NAME PROJECT_NAME

        Generate a project configuration from a blueprint template.

      Options:
        -t, --template TEXT     Blueprint template name  [default: basic_etl]
        --capacity-id TEXT      Fabric capacity ID  [default: ${FABRIC_CAPACITY_ID}]
        --git-repo TEXT         Git repository URL  [default: ${GIT_REPO_URL}]
        --git-branch TEXT       Git branch name  [default: main]
        --output-dir TEXT       Output directory  [default: config/projects/]
        --force                 Overwrite existing configuration
        --help                  Show this message and exit.

      # ls src/usf_fabric_cli/templates/blueprints/ output:
      advanced_analytics.yaml
      basic_etl.yaml
      compliance_regulated.yaml
      data_mesh_domain.yaml
      data_science.yaml
      extensive_example.yaml
      migration_hybrid.yaml
      minimal_starter.yaml
      realtime_streaming.yaml
      specialized_timeseries.yaml

  - id: minimal-starter
    title: "Template 1: minimal_starter - Learning & POCs"
    type: command
    content: |
      **Best For:** Learning Fabric, POCs, Solo projects

      The absolute minimum viable configuration for quick prototyping.

      ## What's Included

      | Resource | Count | Purpose |
      |----------|-------|---------|
      | Lakehouse | 1 | All-in-one data storage |
      | Notebook | 1 | Clear entry point for exploration |
      | Pipeline | 1 | Basic ETL orchestration |
      | Git Integration | No | Simplicity first |

      ## Use Cases

      - Learning Microsoft Fabric fundamentals
      - Individual contributor experiments
      - Quick proof-of-concept development
      - Training environments

      **Est. Cost:** Free on F2 trial, $50-100/month on F8
    code:
      language: bash
      content: |
        # Generate a minimal starter config
        python -m usf_fabric_cli.scripts.dev.generate_project "My Company" "Learning Project" \
          --template minimal_starter

        # View the generated file
        cat config/projects/my_company/learning_project.yaml
    expected_output: |
      âœ“ Configuration generated: config/projects/my_company/learning_project.yaml

      Template: minimal_starter
      Organization: My Company â†’ my_company
      Project: Learning Project â†’ learning_project

      Resources:
        - 1 Lakehouse
        - 1 Notebook
        - 1 Pipeline
    tips:
      - "Graduate to `basic_etl` when you need Git version control"
      - Perfect for weekend learning sessions

  - id: basic-etl
    title: "Template 2: basic_etl - Standard ETL (RECOMMENDED)"
    type: command
    content: |
      **Best For:** Standard ETL pipelines, Team collaboration

      The **recommended starting template** for most production projects.
      Uses the industry-standard medallion architecture (Bronze â†’ Silver â†’ Gold).

      ## What's Included

      | Resource | Count | Purpose |
      |----------|-------|---------|
      | Lakehouses | 3 | Bronze (raw), Silver (cleaned), Gold (curated) |
      | Warehouse | 1 | Structured reporting layer |
      | Notebooks | 3 | Ingestion, Transform, Reporting |
      | Pipelines | 2 | ETL orchestration |
      | Semantic Model | 1 | Power BI dataset |
      | Git Integration | Yes | Version control |

      **Est. Cost:** $100-500/month (F8-F16)
    code:
      language: bash
      content: |
        # Generate a basic ETL config with Git integration
        python -m usf_fabric_cli.scripts.dev.generate_project "Acme Corp" "Sales Analytics" \
          --template basic_etl \
          --git-repo "https://dev.azure.com/acme/FabricProjects/_git/sales"

        # View the generated file
        cat config/projects/acme_corp/sales_analytics.yaml
    expected_output: |
      âœ“ Configuration generated: config/projects/acme_corp/sales_analytics.yaml

      Template: basic_etl
      Organization: Acme Corp â†’ acme_corp
      Project: Sales Analytics â†’ sales_analytics

      Resources:
        - 3 Lakehouses (Bronze, Silver, Gold)
        - 1 Warehouse
        - 3 Notebooks
        - 2 Pipelines
        - 1 Semantic Model

      Git Integration: https://dev.azure.com/acme/FabricProjects/_git/sales
    tips:
      - The medallion architecture is industry standard for data lakes
      - This template is the most commonly deployed

  - id: realtime-streaming
    title: "Template 3: realtime_streaming - IoT & Events"
    type: command
    content: |
      **Best For:** IoT, Events, Real-time analytics

      High-throughput streaming platform with real-time analytics and alerting.

      ## What's Included

      | Resource | Count | Purpose |
      |----------|-------|---------|
      | Lakehouse | 1 | Archival storage |
      | Eventstreams | 3 | Real-time data ingestion |
      | Eventhouse | 1 | Real-time analytics engine |
      | KQL Databases | 2 | Time-series data storage |
      | KQL Queryset | 1 | Reusable analytics queries |
      | Reflex | 2 | Event-driven automation & alerts |
      | KQL Dashboard | 1 | Live monitoring |

      **Est. Cost:** $800-2500/month (F16-F32)

      ## Use Cases

      - IoT device telemetry (1M+ devices)
      - Real-time log aggregation
      - Clickstream analysis
      - Monitoring and alerting systems
    code:
      language: bash
      content: |
        # Generate real-time streaming config
        python -m usf_fabric_cli.scripts.dev.generate_project "TechCorp" "IoT Platform" \
          --template realtime_streaming \
          --capacity-id F32

        # View the generated file
        cat config/projects/techcorp/iot_platform.yaml
    expected_output: |
      âœ“ Configuration generated: config/projects/techcorp/iot_platform.yaml

      Template: realtime_streaming
      Organization: TechCorp â†’ techcorp
      Project: IoT Platform â†’ iot_platform

      Resources:
        - 1 Lakehouse
        - 3 Eventstreams
        - 1 Eventhouse
        - 2 KQL Databases
        - 1 KQL Queryset
        - 2 Reflex
        - 1 KQL Dashboard
    tips:
      - Eventstreams support Azure Event Hubs, IoT Hub, Kafka connectors
      - KQL is optimized for time-series queries at scale

  - id: compliance-regulated
    title: "Template 4: compliance_regulated - Healthcare, Finance, Gov"
    type: command
    content: |
      **Best For:** HIPAA, SOX, GDPR, Government regulations

      Enterprise template with comprehensive audit logging, data classification,
      and regulatory compliance features.

      ## What's Included

      | Resource | Purpose |
      |----------|---------|
      | 4 Lakehouses | PHI Zone, Business Data, Analytics, Audit Trail |
      | 2 Warehouses | Structured reporting with row-level security |
      | Audit Notebooks | Compliance verification scripts |
      | Retention Configs | Data lifecycle management |
      | Access Matrix | Role-based access control templates |

      ## Compliance Features

      - âœ… Data classification tiers (Confidential, Internal, Public)
      - âœ… PHI/PII isolation zones
      - âœ… Audit trail notebooks
      - âœ… Access control matrices
      - âœ… Retention policy templates
      - âœ… Lineage tracking

      **Est. Cost:** $1500-5000/month (F16+)
    code:
      language: bash
      content: |
        # Generate compliance-regulated config
        python -m usf_fabric_cli.scripts.dev.generate_project "HealthCo" "Patient Platform" \
          --template compliance_regulated \
          --capacity-id F64

        # View the generated file
        cat config/projects/healthco/patient_platform.yaml
    expected_output: |
      âœ“ Configuration generated: config/projects/healthco/patient_platform.yaml

      Template: compliance_regulated
      Organization: HealthCo â†’ healthco
      Project: Patient Platform â†’ patient_platform

      Resources:
        - 4 Lakehouses (PHI, Business, Analytics, Audit)
        - 2 Warehouses
        - 6 Notebooks (including audit)
        - 4 Pipelines
        - 2 Semantic Models

      Compliance Features:
        - PHI/PII isolation zones
        - Audit trail logging
        - Data classification tiers
    warnings:
      - Review with your compliance team before deploying to production
      - Additional Azure policies may be required (e.g., Azure Policy, Purview)
      - This template requires proper role assignments for PHI access

  - id: other-templates
    title: "Templates 5-10: Specialized Use Cases"
    type: info
    content: |
      ## Template 5: data_science - Research & ML Experiments

      For data scientists needing experiment tracking and model development.

      ```bash
      python -m usf_fabric_cli.scripts.dev.generate_project "Research Lab" "ML Platform" --template data_science
      ```

      **Includes:** 2 Lakehouses, MLflow integration, Experiment notebooks, Model registry setup

      ---

      ## Template 6: advanced_analytics - ML/AI Production

      For production ML pipelines with model serving and monitoring.

      ```bash
      python -m usf_fabric_cli.scripts.dev.generate_project "AI Corp" "Prediction Engine" --template advanced_analytics
      ```

      **Includes:** 4 Lakehouses, ML pipelines, Model deployment configs, Feature store setup

      ---

      ## Template 7: data_mesh_domain - Domain-Driven Design

      For organizations adopting data mesh architecture with federated ownership.

      ```bash
      python -m usf_fabric_cli.scripts.dev.generate_project "Enterprise" "Finance Domain" --template data_mesh_domain
      ```

      **Includes:** Domain-specific zones, Data products, Self-serve templates, Cross-domain sharing

      ---

      ## Template 8: migration_hybrid - Cloud Migration

      For migrating existing on-premises data warehouses to Fabric.

      ```bash
      python -m usf_fabric_cli.scripts.dev.generate_project "Legacy Corp" "DW Migration" --template migration_hybrid
      ```

      **Includes:** Staging zones, Migration notebooks, Data validation, Legacy connectors

      ---

      ## Template 9: specialized_timeseries - APM & Metrics

      For time-series data, application performance monitoring, and metrics.

      ```bash
      python -m usf_fabric_cli.scripts.dev.generate_project "Ops Team" "APM Platform" --template specialized_timeseries
      ```

      **Includes:** KQL databases, Time-series optimized storage, Alerting, Dashboards

      ---

      ## Template 10: extensive_example - Reference Architecture

      Comprehensive example showing all framework capabilities. Use as learning reference.

      ```bash
      python -m usf_fabric_cli.scripts.dev.generate_project "Demo" "Reference" --template extensive_example
      ```

      **Includes:** Every resource type, All features demonstrated, Documentation examples
    tips:
      - "View any template source: `cat src/usf_fabric_cli/templates/blueprints/<name>.yaml`"
      - All templates can be customized after generation

  - id: generate-command
    title: "Step 2: Generate Your First Configuration"
    type: command
    content: |
      Let's generate a configuration using the recommended `basic_etl` template.

      ## Complete Example with All Options

      This example shows all available command-line options:
    code:
      language: bash
      content: |
        # Full command with all options
        python -m usf_fabric_cli.scripts.dev.generate_project \
          "Global Bank Corp" \
          "Risk Analytics Platform" \
          --template advanced_analytics \
          --capacity-id F128 \
          --git-repo "https://dev.azure.com/globalbank/Fabric/_git/risk-analytics" \
          --git-branch "main" \
          --output-dir "config/projects/" \
          --force  # Overwrite if exists

        # Verify the generated file
        ls -la config/projects/global_bank_corp/

        # View the configuration
        cat config/projects/global_bank_corp/risk_analytics_platform.yaml
    expected_output: |
      âœ“ Configuration generated: config/projects/global_bank_corp/risk_analytics_platform.yaml

      Template: advanced_analytics
      Organization: Global Bank Corp â†’ global_bank_corp
      Project: Risk Analytics Platform â†’ risk_analytics_platform

      Resources:
        - 4 Lakehouses
        - 2 Warehouses
        - 6 Notebooks
        - 4 Pipelines
        - 2 Semantic Models

      Git Integration: https://dev.azure.com/globalbank/Fabric/_git/risk-analytics (branch: main)
      Capacity: F128

      # ls -la output:
      total 8
      drwxrwxr-x 2 user user 4096 Jan 15 10:00 .
      -rw-rw-r-- 1 user user 3456 Jan 15 10:00 risk_analytics_platform.yaml
    tips:
      - Organization and project names are converted to slug format automatically
      - "Use `--force` to regenerate if you need to start over"

  - id: yaml-structure
    title: "Step 3: Understanding the Generated YAML Structure"
    type: config
    content: |
      Let's examine the key sections of a generated configuration file.

      ## Top-Level Structure

      | Section | Purpose |
      |---------|---------|
      | `workspace` | Workspace name, capacity, Git settings |
      | `folders` | Folder structure within the workspace |
      | `lakehouses` | Lakehouse definitions |
      | `warehouses` | Warehouse definitions |
      | `notebooks` | Notebook definitions |
      | `pipelines` | Pipeline definitions |
      | `semantic_models` | Power BI semantic models |
      | `principals` | User/group access assignments |
      | `resources` | Generic resources (any Fabric item type) |
      | `environments` | Environment-specific overrides (dev/staging/prod) |

      ## Key Concepts

      - "**`${VAR_NAME}`** - Environment variable substitution (from `.env`)"
      - "**`{{ env }}`** - Jinja2 template variable (resolved at deploy time)"
      - "**`folder:`** - Places items in a specific folder"
    code:
      language: yaml
      filename: config/projects/acme_corp/sales_analytics.yaml
      content: |
        # Workspace definition
        workspace:
          name: "acme-sales-analytics-{{ env }}"   # Becomes: acme-sales-analytics-dev
          display_name: "Acme Sales Analytics [{{ env | upper }}]"
          description: "Sales analytics platform for Acme Corp"
          capacity_id: "${FABRIC_CAPACITY_ID}"      # From .env file

          # Git integration
          git:
            repository_url: "${GIT_REPO_URL}"
            branch: "main"
            directory: "/"

        # Folder structure
        folders:
          - "01_Bronze"      # Raw data landing zone
          - "02_Silver"      # Cleaned and transformed
          - "03_Gold"        # Business-ready data
          - "99_Notebooks"   # Analysis notebooks

        # Lakehouse definitions
        lakehouses:
          - name: "sales_raw"
            folder: "01_Bronze"
            description: "Raw sales data from source systems"

          - name: "sales_curated"
            folder: "02_Silver"
            description: "Cleaned and transformed sales data"

          - name: "sales_gold"
            folder: "03_Gold"
            description: "Business-ready aggregations"

        # Warehouse definitions
        warehouses:
          - name: "sales_reporting"
            folder: "03_Gold"
            description: "Structured reporting layer for BI tools"

        # Access control
        principals:
          - id: "${DEV_ADMIN_OBJECT_ID}"
            role: "Admin"
          - id: "sales-team@acme.com"
            role: "Contributor"

        # Environment overrides
        environments:
          dev:
            workspace:
              capacity_id: "F8"
          prod:
            workspace:
              capacity_id: "F64"
    tips:
      - "Use `{{ env }}` in workspace names to differentiate environments"
      - "Keep credentials in `.env` using `${VAR}` syntax - never hardcode"

  - id: customization
    title: "Step 4: Customize for Your Organization"
    type: config
    content: |
      After generation, you'll typically want to customize the configuration.

      ## Common Customizations

      | Customization | Example |
      |---------------|---------|
      | **Folder names** | Match your organization's naming conventions |
      | **Lakehouse names** | Use domain-specific prefixes |
      | **Principal assignments** | Add your team's Object IDs or email addresses |
      | **Capacity settings** | Different capacities for dev vs prod |
      | **Git repository** | Your actual Azure DevOps or GitHub URL |
    code:
      language: yaml
      filename: config/projects/acme_corp/manufacturing_analytics.yaml
      content: |
        workspace:
          name: "acme-manufacturing-{{ env }}"
          display_name: "Acme Manufacturing Analytics [{{ env | upper }}]"
          capacity_id: "${FABRIC_CAPACITY_ID}"

        # Custom folder structure for manufacturing domain
        folders:
          - "01_Sensors"         # IoT sensor data
          - "02_Production"      # Production line metrics
          - "03_Quality"         # Quality control data
          - "04_Maintenance"     # Predictive maintenance
          - "99_Analysis"        # Notebooks and reports

        # Domain-specific lakehouse naming
        lakehouses:
          - name: "mfg_sensor_raw"
            folder: "01_Sensors"
            description: "Raw IoT sensor data from production floor"

          - name: "mfg_production_metrics"
            folder: "02_Production"
            description: "Aggregated production line KPIs"

          - name: "mfg_quality_data"
            folder: "03_Quality"
            description: "Quality control measurements and results"

        # Team-specific access control
        principals:
          - id: "${ADMIN_SERVICE_PRINCIPAL}"
            role: "Admin"
          - id: "manufacturing-engineers@acme.com"
            role: "Contributor"
          - id: "quality-control@acme.com"
            role: "Member"
          - id: "plant-managers@acme.com"
            role: "Viewer"

        # Environment-specific overrides
        environments:
          dev:
            workspace:
              capacity_id: "F8"
            principals:
              - id: "${DEV_ADMIN_OBJECT_ID}"
                role: "Admin"

          prod:
            workspace:
              capacity_id: "F64"
            principals:
              - id: "${PROD_ADMIN_OBJECT_ID}"
                role: "Admin"
              - id: "${PROD_READONLY_GROUP}"
                role: "Viewer"
    tips:
      - "The `environments` section lets you override any setting per environment"
      - Object IDs are more reliable than email addresses for automation

  - id: generic-resources
    title: "Step 5: Using Generic Resources (54+ Item Types)"
    type: info
    content: |
      The framework supports **any Fabric item type** through generic resource definitions.
      This future-proofs your configurations as Microsoft adds new item types.

      ## Syntax

      ```yaml
      resources:
        - type: "ItemTypeName"
          name: "item_name"
          folder: "Optional/Folder"
          description: "Optional description"
      ```

      ## Supported Item Types

      | Category | Types |
      |----------|-------|
      | **Real-time** | Eventstream, Eventhouse, Reflex |
      | **KQL** | KQLDatabase, KQLQueryset, KQLDashboard |
      | **ML** | MLModel, MLExperiment, SparkJobDefinition |
      | **Data** | Lakehouse, Warehouse, Dataflow, DataPipeline |
      | **BI** | SemanticModel, Report, Dashboard |
      | **Other** | 40+ more types... |
    code:
      language: yaml
      content: |
        # Generic resource definitions for real-time IoT platform
        resources:
          - type: "Eventstream"
            name: "iot_device_ingestion"
            folder: "01_Sensors"
            description: "Real-time IoT device telemetry stream"

          - type: "Eventhouse"
            name: "sensor_analytics"
            folder: "01_Sensors"
            description: "Real-time analytics engine"

          - type: "KQLDatabase"
            name: "sensor_logs"
            folder: "01_Sensors"
            description: "Time-series sensor data"

          - type: "KQLQueryset"
            name: "sensor_queries"
            folder: "01_Sensors"
            description: "Reusable KQL queries"

          - type: "Reflex"
            name: "maintenance_alerts"
            folder: "04_Maintenance"
            description: "Predictive maintenance alerting"

          - type: "KQLDashboard"
            name: "plant_floor_monitor"
            folder: "99_Analysis"
            description: "Live plant floor monitoring dashboard"
    tips:
      - Check Microsoft Fabric documentation for exact type names
      - Generic resources work with all current and future Fabric item types

  - id: edit-after-generate
    title: "Step 6: Edit Your Generated YAML (The Customization Workflow)"
    type: command
    content: |
      After generating a config, you'll almost always need to customize it.
      Here's the recommended workflow:

      ## The Edit-After-Generate Workflow

      ```
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚                 CONFIGURATION WORKFLOW                          â”‚
      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
      â”‚                                                                 â”‚
      â”‚  1. GENERATE            2. REVIEW              3. CUSTOMIZE     â”‚
      â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€          â”€â”€â”€â”€â”€â”€â”€â”€â”€              â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€     â”‚
      â”‚  generate_project.py    cat config/...yaml    code config/...  â”‚
      â”‚       â†“                      â†“                     â†“           â”‚
      â”‚  Creates starter        See what's there       Edit in VS Code â”‚
      â”‚  from template                                                  â”‚
      â”‚                                                                 â”‚
      â”‚  4. VALIDATE            5. DEPLOY                               â”‚
      â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€          â”€â”€â”€â”€â”€â”€â”€â”€â”€                               â”‚
      â”‚  make validate          make deploy                             â”‚
      â”‚       â†“                      â†“                                  â”‚
      â”‚  Fix any errors         Launch to Fabric                        â”‚
      â”‚                                                                 â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      ```

      ## Common Edits to Make

      | What to Edit | Why | Example |
      |--------------|-----|---------|
      | **principals** | Add YOUR team's Object IDs | `"${DEV_ADMIN_OBJECT_ID}"` |
      | **capacity_id** | Match YOUR Fabric capacity | `"${FABRIC_CAPACITY_ID}"` |
      | **git_repo** | YOUR Azure DevOps/GitHub URL | `"${GIT_REPO_URL}"` |
      | **folders** | Match YOUR naming conventions | `"01_Raw"` vs `"01_Bronze"` |
      | **lakehouse names** | Match YOUR domain terms | `"customer_raw"` not `"sales_raw"` |
      | **notebook content** | YOUR actual logic | Edit file_path or embedded content |
    code:
      language: bash
      content: |
        # Step 1: Generate the starting config
        python -m usf_fabric_cli.scripts.dev.generate_project "Acme Corp" "Sales Analytics" --template basic_etl

        # Step 2: Review what was generated
        cat config/projects/acme_corp/sales_analytics.yaml

        # Step 3: Open in VS Code for editing
        code config/projects/acme_corp/sales_analytics.yaml

        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # NOW MAKE YOUR EDITS IN VS CODE:
        # - Update principals with your team's Object IDs
        # - Verify capacity_id references your .env variable
        # - Customize folder and lakehouse names
        # - Add/remove items as needed
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

        # Step 4: Validate after editing
        make validate config=config/projects/acme_corp/sales_analytics.yaml

        # Step 5: Deploy when validation passes
        make deploy config=config/projects/acme_corp/sales_analytics.yaml env=dev
    expected_output: |
      # After editing and validating:
      Validating configuration: config/projects/acme_corp/sales_analytics.yaml

      âœ“ YAML syntax valid
      âœ“ Required fields present
      âœ“ Environment variables resolved (4 variables)

      â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
      âœ… Configuration is valid!
      â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    tips:
      - "Use VS Code with YAML extension for syntax highlighting"
      - "Always validate after editing - YAML indentation errors are common"
      - "Reference `.env.template` to see all available variables"
    warnings:
      - "Don't edit templates directly - edit the generated file in `config/projects/`"
      - "If you break YAML syntax, `make validate` will tell you the line number"

  - id: validation
    title: "Step 7: Validate Your Configuration"
    type: command
    content: |
      **Always validate your configuration before deploying!**

      Validation checks for:
      - âœ… YAML syntax errors
      - âœ… Required fields present
      - âœ… Environment variables resolvable
      - âœ… Jinja2 template syntax valid
      - âœ… Reference integrity (folders exist for items)
    code:
      language: bash
      content: |
        # Validate using Make (RECOMMENDED)
        make validate config=config/projects/acme_corp/sales_analytics.yaml

        # OR using the CLI directly
        fabric-cicd validate config/projects/acme_corp/sales_analytics.yaml

        # OR using Python module
        python -m usf_fabric_cli.cli validate config/projects/acme_corp/sales_analytics.yaml

        # Validate with specific environment
        fabric-cicd validate config/projects/acme_corp/sales_analytics.yaml --env prod
    expected_output: |
      Validating configuration: config/projects/acme_corp/sales_analytics.yaml

      âœ“ YAML syntax valid
      âœ“ Required fields present
      âœ“ Environment variables resolved (4 variables)
      âœ“ Jinja2 template syntax valid
      âœ“ Folder references valid
      âœ“ No duplicate item names

      â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
      âœ… Configuration is valid!
      â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    tips:
      - Fix ALL validation errors before deploying
      - "Common errors: missing `.env` variables, YAML indentation, duplicate names"
    warnings:
      - Never deploy without validating first
      - CI/CD pipelines should always include a validation step

  - id: next-steps
    title: "ğŸ‰ Ready to Deploy - Next Steps"
    type: info
    content: |
      You've learned how to generate and customize project configurations!

      ## What You've Accomplished

      - âœ… Understand all 10 blueprint templates
      - "âœ… Can use `python -m usf_fabric_cli.scripts.dev.generate_project` with all options"
      - âœ… Know the YAML configuration structure
      - âœ… Can customize configurations for your organization
      - "âœ… Can validate configurations with `make validate`"

      ## Quick Reference: All Templates

      | Template | Command |
      |----------|---------|
      | Learning | `python -m usf_fabric_cli.scripts.dev.generate_project "Org" "Proj" --template minimal_starter` |
      | ETL | `python -m usf_fabric_cli.scripts.dev.generate_project "Org" "Proj" --template basic_etl` |
      | Streaming | `python -m usf_fabric_cli.scripts.dev.generate_project "Org" "Proj" --template realtime_streaming` |
      | Compliance | `python -m usf_fabric_cli.scripts.dev.generate_project "Org" "Proj" --template compliance_regulated` |
      | ML/AI | `python -m usf_fabric_cli.scripts.dev.generate_project "Org" "Proj" --template advanced_analytics` |
      | Data Science | `python -m usf_fabric_cli.scripts.dev.generate_project "Org" "Proj" --template data_science` |
      | Data Mesh | `python -m usf_fabric_cli.scripts.dev.generate_project "Org" "Proj" --template data_mesh_domain` |
      | Migration | `python -m usf_fabric_cli.scripts.dev.generate_project "Org" "Proj" --template migration_hybrid` |
      | Time-series | `python -m usf_fabric_cli.scripts.dev.generate_project "Org" "Proj" --template specialized_timeseries` |
      | Reference | `python -m usf_fabric_cli.scripts.dev.generate_project "Org" "Proj" --template extensive_example` |

      ## Recommended Next Steps

      1. **Next:** [Local Deployment](#local-deployment) - Deploy your configuration to Fabric
      2. **Then:** [Git Integration](#git-integration) - Connect workspace to version control
      3. **Advanced:** [Docker Deployment](#docker-deployment) - CI/CD pipeline integration
    code:
      language: bash
      content: |
        # Generate your first config
        python -m usf_fabric_cli.scripts.dev.generate_project "My Company" "First Project" --template basic_etl

        # Validate it
        make validate config=config/projects/my_company/first_project.yaml

        # Deploy it (next scenario!)
        make deploy config=config/projects/my_company/first_project.yaml env=dev
    tips:
      - Keep your configuration files in version control
      - Use different configs for different projects/teams
