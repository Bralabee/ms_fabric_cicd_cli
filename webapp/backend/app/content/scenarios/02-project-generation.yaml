# Project Generation Scenario
# Comprehensive guide to all 11 templates with exact commands and outputs

id: project-generation
title: Generate Project Configurations from Templates
description: |
  Master the 11 production-ready blueprint templates. Learn the exact `python -m usf_fabric_cli.scripts.dev.generate_project`
  command syntax, understand when to use each template, and customize configurations for your organization's needs.
difficulty: beginner
estimated_duration_minutes: 30
category: configuration
order: 2

prerequisites:
  - getting-started

learning_outcomes:
  - "Use `python -m usf_fabric_cli.scripts.dev.generate_project` with all available options"
  - Choose the right template for your use case from all 11 blueprints
  - Understand the generated YAML configuration structure
  - Customize configurations for your organization
  - Use environment variable substitution in configurations
  - "Validate generated configs with `make validate`"

tags:
  - blueprints
  - templates
  - configuration
  - project-scaffolding
  - yaml
  - generate
  - generate_project.py

related_scenarios:
  - local-deployment
  - git-integration
  - environment-promotion

steps:
  - id: overview
    title: Blueprint Templates Overview
    type: info
    content: |
      The framework includes **11 production-ready blueprint templates** located in `src/usf_fabric_cli/templates/blueprints/`.
      Instead of writing YAML from scratch, you generate configurations using the `generate_project.py` script.

      All templates follow the **Git-sync-only strategy**: the CLI manages the workspace envelope
      (workspace, folders, principals, Git connection, deployment pipeline, folder_rules) while
      Fabric items (lakehouses, notebooks, pipelines) are committed to Git and synced automatically.

      ## The 11 Available Templates

      | Template | Complexity | Best For | Est. Monthly Cost |
      |----------|------------|----------|-------------------|
      | `minimal_starter` | â˜…â˜†â˜†â˜†â˜† | Learning, POCs | $0 (F2 trial) |
      | `basic_etl` | â˜…â˜…â˜†â˜†â˜† | Standard ETL pipelines | $100-500 |
      | `medallion` | â˜…â˜…â˜…â˜†â˜† | Bronze/Silver/Gold data lakes | $200-1000 |
      | `data_science` | â˜…â˜…â˜†â˜†â˜† | Research, ML experiments | $200-800 |
      | `advanced_analytics` | â˜…â˜…â˜…â˜†â˜† | ML/AI production | $500-1500 |
      | `realtime_streaming` | â˜…â˜…â˜…â˜…â˜† | IoT, event streaming | $800-2500 |
      | `data_mesh_domain` | â˜…â˜…â˜…â˜…â˜† | Domain-driven design | $1000-3000 |
      | `migration_hybrid` | â˜…â˜…â˜…â˜…â˜† | Cloud migration | $500-2000 |
      | `specialized_timeseries` | â˜…â˜…â˜…â˜…â˜† | Time-series analytics | $600-2000 |
      | `extensive_example` | â˜…â˜…â˜…â˜…â˜† | Reference architecture | $1000-3000 |
      | `compliance_regulated` | â˜…â˜…â˜…â˜…â˜… | HIPAA, Finance, Gov | $1500-5000 |

      ## Quick Start Recommendation

      - "**New to Fabric?** â†’ Start with `minimal_starter`"
      - "**Production ETL?** â†’ Use `basic_etl` (most common choice)"
      - "**Real-time data?** â†’ Use `realtime_streaming`"
      - "**Regulated industry?** â†’ Use `compliance_regulated`"
    tips:
      - All templates use environment variable substitution for credentials
      - Templates can be customized after generation
      - "View template source: `cat src/usf_fabric_cli/templates/blueprints/<template_name>.yaml`"

  - id: generate-command-syntax
    title: "Step 1: The generate_project.py Command"
    type: command
    content: |
      The `generate_project.py` script is your primary tool for creating configurations.

      ## Command Syntax

      ```bash
      python -m usf_fabric_cli.scripts.dev.generate_project "Organization Name" "Project Name" [OPTIONS]
      ```

      ## All Available Options

      | Option | Description | Default |
      |--------|-------------|---------|
      | `--template`, `-t` | Template name (see list above) | `basic_etl` |
      | `--capacity-id` | Fabric capacity ID | `${FABRIC_CAPACITY_ID}` |
      | `--git-repo` | Git repository URL | `${GIT_REPO_URL}` |
      | `--git-branch` | Git branch name | `main` |
      | `--output-dir` | Output directory | `config/projects/` |
      | `--force` | Overwrite existing | `False` |

      ## Output Location

      Generated files go to: `config/projects/{org_slug}/{project_slug}.yaml`

      - "\"Acme Corp\" â†’ `acme_corp`"
      - "\"Sales Analytics\" â†’ `sales_analytics`"
      - "Result: `config/projects/acme_corp/sales_analytics.yaml`"
    code:
      language: bash
      content: |
        # See all available options
        python -m usf_fabric_cli.scripts.dev.generate_project --help

        # List all available templates
        ls src/usf_fabric_cli/templates/blueprints/
    expected_output: |
      # --help output:
      Usage: generate_project.py [OPTIONS] ORG_NAME PROJECT_NAME

        Generate a project configuration from a blueprint template.

      Options:
        -t, --template TEXT     Blueprint template name  [default: basic_etl]
        --capacity-id TEXT      Fabric capacity ID  [default: ${FABRIC_CAPACITY_ID}]
        --git-repo TEXT         Git repository URL  [default: ${GIT_REPO_URL}]
        --git-branch TEXT       Git branch name  [default: main]
        --output-dir TEXT       Output directory  [default: config/projects/]
        --force                 Overwrite existing configuration
        --help                  Show this message and exit.

      # ls src/usf_fabric_cli/templates/blueprints/ output:
      advanced_analytics.yaml
      basic_etl.yaml
      compliance_regulated.yaml
      data_mesh_domain.yaml
      data_science.yaml
      extensive_example.yaml
      medallion.yaml
      migration_hybrid.yaml
      minimal_starter.yaml
      realtime_streaming.yaml
      specialized_timeseries.yaml

  - id: minimal-starter
    title: "Template 1: minimal_starter - Learning & POCs"
    type: command
    content: |
      **Best For:** Learning Fabric, POCs, Solo projects

      The absolute minimum viable configuration for quick prototyping.
      All templates follow the **Git-sync-only strategy** â€” the CLI creates the workspace
      envelope (workspace, folders, principals, Git connection) while Fabric items are
      committed to Git and synced automatically.

      ## What the Template Configures

      | Component | Details |
      |-----------|---------|
      | Workspace | Named workspace with capacity assignment |
      | Folders | 8 numbered folders (000 Orchestrate â†’ Archive) |
      | Folder Rules | 11 rules mapping item types to folders |
      | Principals | Admin role assignment |
      | Git Integration | Yes (Git-sync-only strategy) |
      | Deployment Pipeline | 3 stages (dev â†’ test â†’ prod) |

      ## Use Cases

      - Learning Microsoft Fabric fundamentals
      - Individual contributor experiments
      - Quick proof-of-concept development
      - Training environments

      **Est. Cost:** Free on F2 trial, $50-100/month on F8
    code:
      language: bash
      content: |
        # Generate a minimal starter config
        python -m usf_fabric_cli.scripts.dev.generate_project "My Company" "Learning Project" \
          --template minimal_starter

        # View the generated file
        cat config/projects/my_company/learning_project.yaml
    expected_output: |
      âœ“ Configuration generated: config/projects/my_company/learning_project.yaml

      Template: minimal_starter
      Organization: My Company â†’ my_company
      Project: Learning Project â†’ learning_project

      Workspace Envelope:
        - 8 Folders (000 Orchestrate â†’ Archive)
        - 11 Folder Rules
        - 1 Principal (Admin)
        - Deployment Pipeline (3 stages)
        - Git Integration enabled
    tips:
      - "Graduate to `basic_etl` for more complex deployments"
      - Perfect for weekend learning sessions

  - id: basic-etl
    title: "Template 2: basic_etl - Standard ETL (RECOMMENDED)"
    type: command
    content: |
      **Best For:** Standard ETL pipelines, Team collaboration

      The **recommended starting template** for most production projects.
      Provides the workspace structure for industry-standard ETL workflows.

      ## What the Template Configures

      | Component | Details |
      |-----------|----------|
      | Workspace | Named workspace with Git connection |
      | Folders | 8 numbered folders (000 Orchestrate â†’ Archive) |
      | Folder Rules | 11 rules (e.g., DataPipelineâ†’000, Lakehouseâ†’200, Notebookâ†’300) |
      | Principals | Admin + Contributor roles |
      | Git Integration | Yes (Git-sync-only strategy) |
      | Deployment Pipeline | 3 stages (dev â†’ test â†’ prod) |

      **Typical Git-managed items:** Lakehouses, Notebooks, Pipelines, Semantic Models

      **Est. Cost:** $100-500/month (F8-F16)
    code:
      language: bash
      content: |
        # Generate a basic ETL config with Git integration
        python -m usf_fabric_cli.scripts.dev.generate_project "Acme Corp" "Sales Analytics" \
          --template basic_etl \
          --git-repo "https://dev.azure.com/acme/FabricProjects/_git/sales"

        # View the generated file
        cat config/projects/acme_corp/sales_analytics.yaml
    expected_output: |
      âœ“ Configuration generated: config/projects/acme_corp/sales_analytics.yaml

      Template: basic_etl
      Organization: Acme Corp â†’ acme_corp
      Project: Sales Analytics â†’ sales_analytics

      Workspace Envelope:
        - 8 Folders (000 Orchestrate â†’ Archive)
        - 11 Folder Rules
        - 2 Principals (Admin, Contributor)
        - Deployment Pipeline (3 stages)

      Git Integration: https://dev.azure.com/acme/FabricProjects/_git/sales
    tips:
      - All items (lakehouses, notebooks, pipelines) are managed through Git sync
      - This template is the most commonly deployed

  - id: realtime-streaming
    title: "Template 3: realtime_streaming - IoT & Events"
    type: command
    content: |
      **Best For:** IoT, Events, Real-time analytics

      High-throughput streaming platform with real-time analytics and alerting.

      ## What's Included

      | Resource | Count | Purpose |
      |----------|-------|---------|
      | Lakehouse | 1 | Archival storage |
      | Eventstreams | 3 | Real-time data ingestion |
      | Eventhouse | 1 | Real-time analytics engine |
      | KQL Databases | 2 | Time-series data storage |
      | KQL Queryset | 1 | Reusable analytics queries |
      | Reflex | 2 | Event-driven automation & alerts |
      | KQL Dashboard | 1 | Live monitoring |

      **Est. Cost:** $800-2500/month (F16-F32)

      ## Use Cases

      - IoT device telemetry (1M+ devices)
      - Real-time log aggregation
      - Clickstream analysis
      - Monitoring and alerting systems
    code:
      language: bash
      content: |
        # Generate real-time streaming config
        python -m usf_fabric_cli.scripts.dev.generate_project "TechCorp" "IoT Platform" \
          --template realtime_streaming \
          --capacity-id F32

        # View the generated file
        cat config/projects/techcorp/iot_platform.yaml
    expected_output: |
      âœ“ Configuration generated: config/projects/techcorp/iot_platform.yaml

      Template: realtime_streaming
      Organization: TechCorp â†’ techcorp
      Project: IoT Platform â†’ iot_platform

      Workspace Envelope:
        - 8 Folders (000 Orchestrate â†’ Archive)
        - 11 Folder Rules
        - Deployment Pipeline (3 stages)

      Capacity: F32
    tips:
      - Eventstreams support Azure Event Hubs, IoT Hub, Kafka connectors
      - KQL is optimized for time-series queries at scale

  - id: compliance-regulated
    title: "Template 4: compliance_regulated - Healthcare, Finance, Gov"
    type: command
    content: |
      **Best For:** HIPAA, SOX, GDPR, Government regulations

      Enterprise template with comprehensive audit logging, data classification,
      and regulatory compliance features.

      ## What's Included

      | Resource | Purpose |
      |----------|---------|
      | 4 Lakehouses | PHI Zone, Business Data, Analytics, Audit Trail |
      | 2 Warehouses | Structured reporting with row-level security |
      | Audit Notebooks | Compliance verification scripts |
      | Retention Configs | Data lifecycle management |
      | Access Matrix | Role-based access control templates |

      ## Compliance Features

      - âœ… Data classification tiers (Confidential, Internal, Public)
      - âœ… PHI/PII isolation zones
      - âœ… Audit trail notebooks
      - âœ… Access control matrices
      - âœ… Retention policy templates
      - âœ… Lineage tracking

      **Est. Cost:** $1500-5000/month (F16+)
    code:
      language: bash
      content: |
        # Generate compliance-regulated config
        python -m usf_fabric_cli.scripts.dev.generate_project "HealthCo" "Patient Platform" \
          --template compliance_regulated \
          --capacity-id F64

        # View the generated file
        cat config/projects/healthco/patient_platform.yaml
    expected_output: |
      âœ“ Configuration generated: config/projects/healthco/patient_platform.yaml

      Template: compliance_regulated
      Organization: HealthCo â†’ healthco
      Project: Patient Platform â†’ patient_platform

      Workspace Envelope:
        - 8 Folders (000 Orchestrate â†’ Archive)
        - 11 Folder Rules
        - 4 Principals (Admin, Contributor, Member, Viewer)
        - Deployment Pipeline (3 stages)

      Compliance Features:
        - PHI/PII isolation zones (via Git-managed items)
        - Audit trail logging
        - Data classification tiers
    warnings:
      - Review with your compliance team before deploying to production
      - Additional Azure policies may be required (e.g., Azure Policy, Purview)
      - This template requires proper role assignments for PHI access

  - id: other-templates
    title: "Templates 5-11: Specialized Use Cases"
    type: info
    content: |
      All templates share the same Git-sync-only structure (8 folders, 11 folder_rules,
      deployment pipeline, Git integration). They differ in descriptions, domain guidance,
      and comments listing typical Git-managed items for the use case.

      ## Template 5: medallion - Bronze/Silver/Gold Data Lakes

      Multi-layer data platform with the medallion architecture pattern.

      ```bash
      python -m usf_fabric_cli.scripts.dev.generate_project "Data Corp" "Data Lake" --template medallion
      ```

      **Configures:** 8 numbered folders, 11 folder_rules. **Typical Git items:** Bronze/Silver/Gold Lakehouses, transformation Notebooks, orchestration Pipelines

      ---

      ## Template 6: data_science - Research & ML Experiments

      For data scientists needing experiment tracking and model development.

      ```bash
      python -m usf_fabric_cli.scripts.dev.generate_project "Research Lab" "ML Platform" --template data_science
      ```

      **Configures:** 8 numbered folders, 11 folder_rules. **Typical Git items:** Experiment Lakehouses, ML Notebooks, Model registry items

      ---

      ## Template 7: advanced_analytics - ML/AI Production

      For production ML pipelines with model serving and monitoring.

      ```bash
      python -m usf_fabric_cli.scripts.dev.generate_project "AI Corp" "Prediction Engine" --template advanced_analytics
      ```

      **Configures:** 8 numbered folders, 11 folder_rules. **Typical Git items:** Feature store Lakehouses, ML Pipelines, Model serving configs

      ---

      ## Template 8: data_mesh_domain - Domain-Driven Design

      For organizations adopting data mesh architecture with federated ownership.

      ```bash
      python -m usf_fabric_cli.scripts.dev.generate_project "Enterprise" "Finance Domain" --template data_mesh_domain
      ```

      **Configures:** 8 numbered folders, 11 folder_rules. **Typical Git items:** Domain-specific data products, cross-domain sharing configs

      ---

      ## Template 9: migration_hybrid - Cloud Migration

      For migrating existing on-premises data warehouses to Fabric.

      ```bash
      python -m usf_fabric_cli.scripts.dev.generate_project "Legacy Corp" "DW Migration" --template migration_hybrid
      ```

      **Configures:** 8 numbered folders, 11 folder_rules. **Typical Git items:** Migration Notebooks, validation Pipelines, staging Lakehouses

      ---

      ## Template 10: specialized_timeseries - APM & Metrics

      For time-series data, application performance monitoring, and metrics.

      ```bash
      python -m usf_fabric_cli.scripts.dev.generate_project "Ops Team" "APM Platform" --template specialized_timeseries
      ```

      **Configures:** 8 numbered folders, 11 folder_rules. **Typical Git items:** KQL Databases, Eventstreams, KQL Dashboards, Reflex alerts

      ---

      ## Template 11: extensive_example - Reference Architecture

      Comprehensive example showing all framework capabilities. Use as learning reference.

      ```bash
      python -m usf_fabric_cli.scripts.dev.generate_project "Demo" "Reference" --template extensive_example
      ```

      **Configures:** 8 numbered folders, 11 folder_rules. **Typical Git items:** Every resource type documented in comments
    tips:
      - "View any template source: `cat src/usf_fabric_cli/templates/blueprints/<name>.yaml`"
      - All templates can be customized after generation

  - id: generate-command
    title: "Step 2: Generate Your First Configuration"
    type: command
    content: |
      Let's generate a configuration using the recommended `basic_etl` template.

      ## Complete Example with All Options

      This example shows all available command-line options:
    code:
      language: bash
      content: |
        # Full command with all options
        python -m usf_fabric_cli.scripts.dev.generate_project \
          "Global Bank Corp" \
          "Risk Analytics Platform" \
          --template advanced_analytics \
          --capacity-id F128 \
          --git-repo "https://dev.azure.com/globalbank/Fabric/_git/risk-analytics" \
          --git-branch "main" \
          --output-dir "config/projects/" \
          --force  # Overwrite if exists

        # Verify the generated file
        ls -la config/projects/global_bank_corp/

        # View the configuration
        cat config/projects/global_bank_corp/risk_analytics_platform.yaml
    expected_output: |
      âœ“ Configuration generated: config/projects/global_bank_corp/risk_analytics_platform.yaml

      Template: advanced_analytics
      Organization: Global Bank Corp â†’ global_bank_corp
      Project: Risk Analytics Platform â†’ risk_analytics_platform

      Workspace Envelope:
        - 8 Folders (000 Orchestrate â†’ Archive)
        - 11 Folder Rules
        - Deployment Pipeline (3 stages)

      Git Integration: https://dev.azure.com/globalbank/Fabric/_git/risk-analytics (branch: main)
      Capacity: F128

      # ls -la output:
      total 8
      drwxrwxr-x 2 user user 4096 Jan 15 10:00 .
      -rw-rw-r-- 1 user user 3456 Jan 15 10:00 risk_analytics_platform.yaml
    tips:
      - Organization and project names are converted to slug format automatically
      - "Use `--force` to regenerate if you need to start over"

  - id: yaml-structure
    title: "Step 3: Understanding the Generated YAML Structure"
    type: config
    content: |
      Let's examine the key sections of a generated configuration file.

      ## Top-Level Structure

      | Section | Purpose |
      |---------|---------|
      | `workspace` | Workspace name, capacity, Git settings |
      | `folders` | 8 numbered folders within the workspace |
      | `folder_rules` | Maps item types to folders automatically |
      | `principals` | User/group access assignments |
      | `deployment_pipeline` | 3-stage promotion pipeline (dev/test/prod) |
      | `lakehouses` | Lakehouse definitions (empty â€” Git-sync-only) |
      | `notebooks` | Notebook definitions (empty â€” Git-sync-only) |
      | `resources` | Generic resources (empty â€” Git-sync-only) |
      | `environments` | Environment-specific overrides (dev/staging/prod) |

      ## Key Concepts

      - "**`${VAR_NAME}`** - Environment variable substitution (from `.env`)"
      - "**`{{ env }}`** - Jinja2 template variable (resolved at deploy time)"
      - "**Git-sync-only** - Items managed through Git, not CLI"
    code:
      language: yaml
      filename: config/projects/acme_corp/sales_analytics.yaml
      content: |
        # Workspace definition
        workspace:
          name: "acme-sales-analytics-{{ env }}"   # Becomes: acme-sales-analytics-dev
          display_name: "Acme Sales Analytics [{{ env | upper }}]"
          description: "Sales analytics platform for Acme Corp"
          capacity_id: "${FABRIC_CAPACITY_ID}"      # From .env file

          # Git integration
          git:
            repository_url: "${GIT_REPO_URL}"
            branch: "main"
            directory: "/"

        # 8 Numbered folder structure (all templates use these)
        folders:
          - "000 Orchestrate"   # DataPipeline, DataflowGen2
          - "100 Ingest"        # Eventstream
          - "200 Store"         # Lakehouse
          - "300 Prepare"       # Notebook, SparkJobDefinition
          - "400 Model"         # Warehouse, SemanticModel
          - "500 Visualize"     # Report, Dashboard
          - "999 Libraries"     # Environment
          - "Archive"           # Archived items

        # Folder rules â€” auto-organize items by type
        folder_rules:
          DataPipeline: "000 Orchestrate"
          DataflowGen2: "000 Orchestrate"
          Eventstream: "100 Ingest"
          Lakehouse: "200 Store"
          Notebook: "300 Prepare"
          SparkJobDefinition: "300 Prepare"
          Warehouse: "400 Model"
          SemanticModel: "400 Model"
          Report: "500 Visualize"
          Dashboard: "500 Visualize"
          Environment: "999 Libraries"

        # Git-sync-only: items managed through Git, not CLI
        lakehouses: []    # Typical: sales_raw, sales_curated, sales_gold
        notebooks: []     # Typical: ingestion, transform, reporting
        resources: []     # Typical: Pipelines, Semantic Models

        # Deployment pipeline
        deployment_pipeline:
          name: "acme-sales-analytics-pipeline"
          stages:
            - name: "development"
              workspace: "acme-sales-analytics-dev"
            - name: "test"
              workspace: "acme-sales-analytics-test"
            - name: "production"
              workspace: "acme-sales-analytics-prod"

        # Access control
        principals:
          - id: "${DEV_ADMIN_OBJECT_ID}"
            role: "Admin"
          - id: "sales-team@acme.com"
            role: "Contributor"

        # Environment overrides
        environments:
          dev:
            workspace:
              capacity_id: "F8"
          prod:
            workspace:
              capacity_id: "F64"
    tips:
      - "Use `{{ env }}` in workspace names to differentiate environments"
      - "Keep credentials in `.env` using `${VAR}` syntax - never hardcode"

  - id: customization
    title: "Step 4: Customize for Your Organization"
    type: config
    content: |
      After generation, you'll typically want to customize the configuration.

      ## Common Customizations

      | Customization | Example |
      |---------------|---------|
      | **Principal assignments** | Add your team's Object IDs or email addresses |
      | **Capacity settings** | Different capacities for dev vs prod |
      | **Git repository** | Your actual Azure DevOps or GitHub URL |
      | **Deployment pipeline** | Customize stage names and workspace mappings |
      | **Description** | Domain-specific workspace descriptions |
    code:
      language: yaml
      filename: config/projects/acme_corp/manufacturing_analytics.yaml
      content: |
        workspace:
          name: "acme-manufacturing-{{ env }}"
          display_name: "Acme Manufacturing Analytics [{{ env | upper }}]"
          capacity_id: "${FABRIC_CAPACITY_ID}"

          git:
            repository_url: "${GIT_REPO_URL}"
            branch: "main"
            directory: "/"

        # Standard 8 numbered folders (same across all templates)
        folders:
          - "000 Orchestrate"
          - "100 Ingest"
          - "200 Store"
          - "300 Prepare"
          - "400 Model"
          - "500 Visualize"
          - "999 Libraries"
          - "Archive"

        # Folder rules map item types to folders
        folder_rules:
          DataPipeline: "000 Orchestrate"
          DataflowGen2: "000 Orchestrate"
          Eventstream: "100 Ingest"
          Lakehouse: "200 Store"
          Notebook: "300 Prepare"
          SparkJobDefinition: "300 Prepare"
          Warehouse: "400 Model"
          SemanticModel: "400 Model"
          Report: "500 Visualize"
          Dashboard: "500 Visualize"
          Environment: "999 Libraries"

        # Git-sync-only: items committed to Git, not created by CLI
        lakehouses: []    # e.g., mfg_sensor_raw, mfg_production_metrics
        notebooks: []     # e.g., sensor_ingestion, quality_analysis
        resources: []     # e.g., Eventstreams, KQL Databases

        # Team-specific access control
        principals:
          - id: "${ADMIN_SERVICE_PRINCIPAL}"
            role: "Admin"
          - id: "manufacturing-engineers@acme.com"
            role: "Contributor"
          - id: "quality-control@acme.com"
            role: "Member"
          - id: "plant-managers@acme.com"
            role: "Viewer"

        # Deployment pipeline
        deployment_pipeline:
          name: "acme-manufacturing-pipeline"
          stages:
            - name: "development"
              workspace: "acme-manufacturing-dev"
            - name: "test"
              workspace: "acme-manufacturing-test"
            - name: "production"
              workspace: "acme-manufacturing-prod"

        # Environment-specific overrides
        environments:
          dev:
            workspace:
              capacity_id: "F8"
            principals:
              - id: "${DEV_ADMIN_OBJECT_ID}"
                role: "Admin"

          prod:
            workspace:
              capacity_id: "F64"
            principals:
              - id: "${PROD_ADMIN_OBJECT_ID}"
                role: "Admin"
              - id: "${PROD_READONLY_GROUP}"
                role: "Viewer"
    tips:
      - "The `environments` section lets you override any setting per environment"
      - Object IDs are more reliable than email addresses for automation

  - id: generic-resources
    title: "Step 5: Using Generic Resources (54+ Item Types)"
    type: info
    content: |
      The framework supports **any Fabric item type** through generic resource definitions.
      This future-proofs your configurations as Microsoft adds new item types.

      ## Syntax

      ```yaml
      resources:
        - type: "ItemTypeName"
          name: "item_name"
          folder: "Optional/Folder"
          description: "Optional description"
      ```

      ## Supported Item Types

      | Category | Types |
      |----------|-------|
      | **Real-time** | Eventstream, Eventhouse, Reflex |
      | **KQL** | KQLDatabase, KQLQueryset, KQLDashboard |
      | **ML** | MLModel, MLExperiment, SparkJobDefinition |
      | **Data** | Lakehouse, Warehouse, Dataflow, DataPipeline |
      | **BI** | SemanticModel, Report, Dashboard |
      | **Other** | 40+ more types... |
    code:
      language: yaml
      content: |
        # Generic resource definitions (for reference â€” templates use Git-sync-only)
        # If you need to define items directly, use this syntax:
        resources:
          - type: "Eventstream"
            name: "iot_device_ingestion"
            description: "Real-time IoT device telemetry stream"

          - type: "Eventhouse"
            name: "sensor_analytics"
            description: "Real-time analytics engine"

          - type: "KQLDatabase"
            name: "sensor_logs"
            description: "Time-series sensor data"

          - type: "KQLQueryset"
            name: "sensor_queries"
            description: "Reusable KQL queries"

          - type: "Reflex"
            name: "maintenance_alerts"
            description: "Predictive maintenance alerting"

          - type: "KQLDashboard"
            name: "plant_floor_monitor"
            description: "Live plant floor monitoring dashboard"
    tips:
      - "Recommended: use Git-sync-only (empty arrays) and manage items through Git"
      - Generic resources work with all current and future Fabric item types

  - id: edit-after-generate
    title: "Step 6: Edit Your Generated YAML (The Customization Workflow)"
    type: command
    content: |
      After generating a config, you'll almost always need to customize it.
      Here's the recommended workflow:

      ## The Edit-After-Generate Workflow

      ```
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚                 CONFIGURATION WORKFLOW                          â”‚
      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
      â”‚                                                                 â”‚
      â”‚  1. GENERATE            2. REVIEW              3. CUSTOMIZE     â”‚
      â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€          â”€â”€â”€â”€â”€â”€â”€â”€â”€              â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€     â”‚
      â”‚  generate_project.py    cat config/...yaml    code config/...  â”‚
      â”‚       â†“                      â†“                     â†“           â”‚
      â”‚  Creates starter        See what's there       Edit in VS Code â”‚
      â”‚  from template                                                  â”‚
      â”‚                                                                 â”‚
      â”‚  4. VALIDATE            5. DEPLOY                               â”‚
      â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€          â”€â”€â”€â”€â”€â”€â”€â”€â”€                               â”‚
      â”‚  make validate          make deploy                             â”‚
      â”‚       â†“                      â†“                                  â”‚
      â”‚  Fix any errors         Launch to Fabric                        â”‚
      â”‚                                                                 â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      ```

      ## Common Edits to Make

      | What to Edit | Why | Example |
      |--------------|-----|---------|
      | **principals** | Add YOUR team's Object IDs | `"${DEV_ADMIN_OBJECT_ID}"` |
      | **capacity_id** | Match YOUR Fabric capacity | `"${FABRIC_CAPACITY_ID}"` |
      | **git_repo** | YOUR Azure DevOps/GitHub URL | `"${GIT_REPO_URL}"` |
      | **deployment_pipeline** | YOUR workspace names per stage | Custom stage mappings |
      | **description** | YOUR domain-specific descriptions | Workspace + pipeline descriptions |
    code:
      language: bash
      content: |
        # Step 1: Generate the starting config
        python -m usf_fabric_cli.scripts.dev.generate_project "Acme Corp" "Sales Analytics" --template basic_etl

        # Step 2: Review what was generated
        cat config/projects/acme_corp/sales_analytics.yaml

        # Step 3: Open in VS Code for editing
        code config/projects/acme_corp/sales_analytics.yaml

        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # NOW MAKE YOUR EDITS IN VS CODE:
        # - Update principals with your team's Object IDs
        # - Verify capacity_id references your .env variable
        # - Customize folder and lakehouse names
        # - Add/remove items as needed
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

        # Step 4: Validate after editing
        make validate config=config/projects/acme_corp/sales_analytics.yaml

        # Step 5: Deploy when validation passes
        make deploy config=config/projects/acme_corp/sales_analytics.yaml env=dev
    expected_output: |
      # After editing and validating:
      Validating configuration: config/projects/acme_corp/sales_analytics.yaml

      âœ“ YAML syntax valid
      âœ“ Required fields present
      âœ“ Environment variables resolved (4 variables)

      â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
      âœ… Configuration is valid!
      â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    tips:
      - "Use VS Code with YAML extension for syntax highlighting"
      - "Always validate after editing - YAML indentation errors are common"
      - "Reference `.env.template` to see all available variables"
    warnings:
      - "Don't edit templates directly - edit the generated file in `config/projects/`"
      - "If you break YAML syntax, `make validate` will tell you the line number"

  - id: validation
    title: "Step 7: Validate Your Configuration"
    type: command
    content: |
      **Always validate your configuration before deploying!**

      Validation checks for:
      - âœ… YAML syntax errors
      - âœ… Required fields present
      - âœ… Environment variables resolvable
      - âœ… Jinja2 template syntax valid
      - âœ… Reference integrity (folders exist for items)
    code:
      language: bash
      content: |
        # Validate using Make (RECOMMENDED)
        make validate config=config/projects/acme_corp/sales_analytics.yaml

        # OR using the CLI directly
        fabric-cicd validate config/projects/acme_corp/sales_analytics.yaml

        # OR using Python module
        python -m usf_fabric_cli.cli validate config/projects/acme_corp/sales_analytics.yaml

        # Validate with specific environment
        fabric-cicd validate config/projects/acme_corp/sales_analytics.yaml --env prod
    expected_output: |
      Validating configuration: config/projects/acme_corp/sales_analytics.yaml

      âœ“ YAML syntax valid
      âœ“ Required fields present
      âœ“ Environment variables resolved (4 variables)
      âœ“ Jinja2 template syntax valid
      âœ“ Folder references valid
      âœ“ No duplicate item names

      â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
      âœ… Configuration is valid!
      â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    tips:
      - Fix ALL validation errors before deploying
      - "Common errors: missing `.env` variables, YAML indentation, duplicate names"
    warnings:
      - Never deploy without validating first
      - CI/CD pipelines should always include a validation step

  - id: next-steps
    title: "ğŸ‰ Ready to Deploy - Next Steps"
    type: info
    content: |
      You've learned how to generate and customize project configurations!

      ## What You've Accomplished

      - âœ… Understand all 11 blueprint templates and the Git-sync-only strategy
      - "âœ… Can use `python -m usf_fabric_cli.scripts.dev.generate_project` with all options"
      - âœ… Know the YAML configuration structure (workspace, folders, folder_rules, principals, pipeline)
      - âœ… Can customize configurations for your organization
      - "âœ… Can validate configurations with `make validate`"

      ## Quick Reference: All Templates

      | Template | Command |
      |----------|---------|
      | Learning | `python -m usf_fabric_cli.scripts.dev.generate_project "Org" "Proj" --template minimal_starter` |
      | ETL | `python -m usf_fabric_cli.scripts.dev.generate_project "Org" "Proj" --template basic_etl` |
      | Medallion | `python -m usf_fabric_cli.scripts.dev.generate_project "Org" "Proj" --template medallion` |
      | Streaming | `python -m usf_fabric_cli.scripts.dev.generate_project "Org" "Proj" --template realtime_streaming` |
      | Compliance | `python -m usf_fabric_cli.scripts.dev.generate_project "Org" "Proj" --template compliance_regulated` |
      | ML/AI | `python -m usf_fabric_cli.scripts.dev.generate_project "Org" "Proj" --template advanced_analytics` |
      | Data Science | `python -m usf_fabric_cli.scripts.dev.generate_project "Org" "Proj" --template data_science` |
      | Data Mesh | `python -m usf_fabric_cli.scripts.dev.generate_project "Org" "Proj" --template data_mesh_domain` |
      | Migration | `python -m usf_fabric_cli.scripts.dev.generate_project "Org" "Proj" --template migration_hybrid` |
      | Time-series | `python -m usf_fabric_cli.scripts.dev.generate_project "Org" "Proj" --template specialized_timeseries` |
      | Reference | `python -m usf_fabric_cli.scripts.dev.generate_project "Org" "Proj" --template extensive_example` |

      ## Recommended Next Steps

      1. **Next:** [Local Deployment](#local-deployment) - Deploy your configuration to Fabric
      2. **Then:** [Git Integration](#git-integration) - Connect workspace to version control
      3. **Advanced:** [Docker Deployment](#docker-deployment) - CI/CD pipeline integration
    code:
      language: bash
      content: |
        # Generate your first config
        python -m usf_fabric_cli.scripts.dev.generate_project "My Company" "First Project" --template basic_etl

        # Validate it
        make validate config=config/projects/my_company/first_project.yaml

        # Deploy it (next scenario!)
        make deploy config=config/projects/my_company/first_project.yaml env=dev
    tips:
      - Keep your configuration files in version control
      - Use different configs for different projects/teams
